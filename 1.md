
# Supervised learning

Given a training set, learn a good predictor.

- regression problem: target variable continuous
- classification problem: discrete

## Linear regression

$$h(x) = \theta_0 + \theta_1x_1 + \theta_2x_2 =
\sum_{i=0}^n\theta_ix_i=\theta^Tx$$

cost function

$$J(\theta)=\frac{1}{2}\sum_{i=1}^m(h_\theta(x^{(i)}-y^{(i)})^2.$$

### LMS algorithm

Gradient descent algorithm: start with some guess, update $\theta$ by partial
derivative multiplied by learning rate $\alpha$.

Linear regression problem has only one global optima, therefore gradiant desent
always converges.

batch v.s incremental gradient desent, when the training set is large, latter is
preferred.

### Vectorized solution

$$J(\theta)=\frac{1}{2}(X\theta-\overrightarrow{y})^T(X\theta-\overrightarrow{y}
)$$

$$\nabla_\theta J(\theta)=X^TX\theta-X^T\overrightarrow{y}$$

To minimize $J$, we set its derivative to zero, so
$\theta=(X^TX)^{-1}X^T\overrightarrow{y}$

## Classification problem

### Logistic regression

We want the possible values of the hypotheses to be discrete, so we replace the
hypotheses with

$$h_\theta(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}$$

where $$g(z)=\frac{1}{1+e^{-z}}$$

### Newton's method

Newton's method

$$\theta:=\theta-\frac{f(\theta)}{f'(\theta)}$$

is generalized to Newton-Raphson method to fit in this multidimensional settting

$$\theta:=\theta-H^{-1}\nabla_\theta l(\theta)$$

Newton's method typically coverges faster than gradient descent.


    

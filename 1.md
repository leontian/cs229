
# Supervised learning

Given a training set, learn a good predictor.

- regression problem: target variable continuous
- classification problem: discrete

## Linear regression

$$h(x) = \theta_0 + \theta_1x_1 + \theta_2x_2 =
\sum_{i=0}^n\theta_ix_i=\theta^Tx$$

cost function

$$J(\theta)=\frac{1}{2}\sum_{i=1}^m(h_\theta(x^{(i)}-y^{(i)})^2.$$

### LMS algorithm

Gradient descent algorithm: start with some guess, update $\theta$ by partial
derivative multiplied by learning rate $\alpha$.

Linear regression problem has only one global optima, therefore gradiant desent
always converges.

batch v.s incremental gradient desent, when the training set is large, latter is
preferred.

### Vectorized solution

$$J(\theta)=\frac{1}{2}(X\theta-\overrightarrow{y})^T(X\theta-\overrightarrow{y}
)$$

$$\nabla_\theta J(\theta)=X^TX\theta-X^T\overrightarrow{y}$$

To minimize $J$, we set its derivative to zero, so
$\theta=(X^TX)^{-1}X^T\overrightarrow{y}$

## Classification problem

### Logistic regression

We want the possible values of the hypotheses to be discrete, so we replace the
hypotheses with

$$h_\theta(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}$$

where $$g(z)=\frac{1}{1+e^{-z}}$$

### Newton's method

By applying Newton's method

$$\theta:=\theta-\frac{f(\theta)}{f'(\theta)}$$

to the first derivative of a given funtion, we get the maxima. This is
generalized to Newton-Raphson method to fit in this multidimensional settting

$$\theta:=\theta-H^{-1}\nabla_\theta l(\theta)$$

where $H$ is the Hessian.

Newton's method typically coverges faster than gradient descent.

## GLM

The exponetial family of probablity distributions are those can be written in
the form

$$p(y;\eta) = b(y)\exp(\eta^TT(y) - a(\eta)$$

Bernoulli and Gaussian distributions are both example of this exponential
family.

- Bernoulli distribution: $$p(y;\phi)=\phi^y(1-\phi)^{1-y}$$
- Gaussian distribution (Normal distribution):
$$p(y;\mu)=\frac{1}{\sqrt{2\pi\sigma^2}}\exp(-\frac{(y-\mu)^2}{2\sigma^2})$$
- and more: multinomial, Poisson, gamma, exponotial, beta, Dirichlet.

where $\mu$ is the mean and $\sigma$ is the standard deviation, whence it's
variance is $\sigma^2$.

### Construction GLMs

Suppose we would like to predict the value of some variable $y$ as a function of
$x$ in a classification or regression problem. We need the following three
assumptions about the conditional distribution of $y$ given $x$ and about the
model.

1. Exponential family distribution, $y|x;\theta \sim
\text{ExponentialFamily}(\eta)$
2. Our goal is to predict expected value of $T(y)$, which in most of our
examples is equal to $y$, this means we would like our prediction $h(x)=E[y|x]$.
3. $\eta$, the natural parameter, and $x$ are related linearly:
$\eta=\theta^Tx$.

Both logistic regression and ordinary least squares can be derived as GLMs.




    

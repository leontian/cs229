{
 "metadata": {
  "name": "",
  "signature": "sha256:8c551ae3641c6946cd25f3af3ad81a945671b2f2f44290cba0e479f7392e105b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Supervised learning\n",
      "\n",
      "Given a training set, learn a good predictor.\n",
      "\n",
      "- regression problem: target variable continuous\n",
      "- classification problem: discrete\n",
      "\n",
      "## Linear regression\n",
      "\n",
      "$$h(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 = \\sum_{i=0}^n\\theta_ix_i=\\theta^Tx$$\n",
      "\n",
      "cost function\n",
      "\n",
      "$$J(\\theta)=\\frac{1}{2}\\sum_{i=1}^m(h_\\theta(x^{(i)}-y^{(i)})^2.$$\n",
      "\n",
      "### LMS algorithm\n",
      "\n",
      "Gradient descent algorithm: start with some guess, update $\\theta$ by partial derivative multiplied by learning rate $\\alpha$.\n",
      "\n",
      "Linear regression problem has only one global optima, therefore gradiant desent always converges.\n",
      "\n",
      "batch v.s incremental gradient desent, when the training set is large, latter is preferred.\n",
      "\n",
      "### Vectorized solution\n",
      "\n",
      "$$J(\\theta)=\\frac{1}{2}(X\\theta-\\overrightarrow{y})^T(X\\theta-\\overrightarrow{y})$$\n",
      "\n",
      "$$\\nabla_\\theta J(\\theta)=X^TX\\theta-X^T\\overrightarrow{y}$$\n",
      "\n",
      "To minimize $J$, we set its derivative to zero, so $\\theta=(X^TX)^{-1}X^T\\overrightarrow{y}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Classification problem\n",
      "\n",
      "### Logistic regression\n",
      "\n",
      "We want the possible values of the hypotheses to be discrete, so we replace the hypotheses with\n",
      "\n",
      "$$h_\\theta(x)=g(\\theta^Tx)=\\frac{1}{1+e^{-\\theta^Tx}}$$\n",
      "\n",
      "where $$g(z)=\\frac{1}{1+e^{-z}}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Newton's method\n",
      "\n",
      "By applying Newton's method\n",
      "\n",
      "$$\\theta:=\\theta-\\frac{f(\\theta)}{f'(\\theta)}$$\n",
      "\n",
      "to the first derivative of a given funtion, we get the maxima. This is generalized to Newton-Raphson method to fit in this multidimensional settting\n",
      "\n",
      "$$\\theta:=\\theta-H^{-1}\\nabla_\\theta l(\\theta)$$\n",
      "\n",
      "where $H$ is the Hessian.\n",
      "\n",
      "Newton's method typically coverges faster than gradient descent."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## GLM\n",
      "\n",
      "The exponetial family of probablity distributions are those can be written in the form\n",
      "\n",
      "$$p(y;\\eta) = b(y)\\exp(\\eta^TT(y) - a(\\eta)$$\n",
      "\n",
      "Bernoulli and Gaussian distributions are both example of this exponential family.\n",
      "\n",
      "- Bernoulli distribution: $$p(y;\\phi)=\\phi^y(1-\\phi)^{1-y}$$\n",
      "- Gaussian distribution (Normal distribution): $$p(y;\\mu)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp(-\\frac{(y-\\mu)^2}{2\\sigma^2})$$\n",
      "- and more: multinomial, Poisson, gamma, exponotial, beta, Dirichlet.\n",
      "\n",
      "where $\\mu$ is the mean and $\\sigma$ is the standard deviation, whence it's variance is $\\sigma^2$.\n",
      "\n",
      "### Construction GLMs\n",
      "\n",
      "Suppose we would like to predict the value of some variable $y$ as a function of $x$ in a classification or regression problem. We need the following three assumptions about the conditional distribution of $y$ given $x$ and about the model.\n",
      "\n",
      "1. Exponential family distribution, $y|x;\\theta \\sim \\text{ExponentialFamily}(\\eta)$\n",
      "2. Our goal is to predict expected value of $T(y)$, which in most of our examples is equal to $y$, this means we would like our prediction $h(x)=E[y|x]$.\n",
      "3. $\\eta$, the natural parameter, and $x$ are related linearly: $\\eta=\\theta^Tx$.\n",
      "\n",
      "Both logistic regression and ordinary least squares can be derived as GLMs.\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
{
 "metadata": {
  "name": "",
  "signature": "sha256:e003ac48edc4b51d3308423630c4191ac17b483719e7020e796e05e1ba73d7fc"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Supervised learning\n",
      "\n",
      "Given a training set, learn a good predictor.\n",
      "\n",
      "- regression problem: target variable continuous\n",
      "- classification problem: discrete\n",
      "\n",
      "## Linear regression\n",
      "\n",
      "$$h(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 = \\sum_{i=0}^n\\theta_ix_i=\\theta^Tx$$\n",
      "\n",
      "cost function\n",
      "\n",
      "$$J(\\theta)=\\frac{1}{2}\\sum_{i=1}^m(h_\\theta(x^{(i)}-y^{(i)})^2.$$\n",
      "\n",
      "### LMS algorithm\n",
      "\n",
      "Gradient descent algorithm: start with some guess, update $\\theta$ by partial derivative multiplied by learning rate $\\alpha$.\n",
      "\n",
      "Linear regression problem has only one global optima, therefore gradiant desent always converges.\n",
      "\n",
      "batch v.s incremental gradient desent, when the training set is large, latter is preferred.\n",
      "\n",
      "### Vectorized solution\n",
      "\n",
      "$$J(\\theta)=\\frac{1}{2}(X\\theta-\\overrightarrow{y})^T(X\\theta-\\overrightarrow{y})$$\n",
      "\n",
      "$$\\nabla_\\theta J(\\theta)=X^TX\\theta-X^T\\overrightarrow{y}$$\n",
      "\n",
      "To minimize $J$, we set its derivative to zero, so $\\theta=(X^TX)^{-1}X^T\\overrightarrow{y}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Classification problem\n",
      "\n",
      "### Logistic regression\n",
      "\n",
      "We want the possible values of the hypotheses to be discrete, so we replace the hypotheses with\n",
      "\n",
      "$$h_\\theta(x)=g(\\theta^Tx)=\\frac{1}{1+e^{-\\theta^Tx}}$$\n",
      "\n",
      "where $$g(z)=\\frac{1}{1+e^{-z}}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Newton's method\n",
      "\n",
      "Newton's method\n",
      "\n",
      "$$\\theta:=\\theta-\\frac{f(\\theta)}{f'(\\theta)}$$\n",
      "\n",
      "is generalized to Newton-Raphson method to fit in this multidimensional settting\n",
      "\n",
      "$$\\theta:=\\theta-H^{-1}\\nabla_\\theta l(\\theta)$$\n",
      "\n",
      "Newton's method typically coverges faster than gradient descent."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}